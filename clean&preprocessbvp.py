# -*- coding: utf-8 -*-
"""clean&preprocessBVP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sPG_-h09iIWI2dcQX0-ZptvKxTJlwUMi
"""

import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/dataset/self/P4.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.head()

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P4_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P5.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P5_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P8.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P8_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P9.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P9_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P10.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P10.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

# Step 3: Create column 'emotion1' based on the presence of 1 in specific columns
emotion1_columns = ['boredom', 'confusion', 'delight', 'concentration', 'frustration', 'surprise', 'none_1']
df['emotion1'] = df[emotion1_columns].idxmax(axis=1)

# Replace 'none_1' with 'no emotion'
df['emotion1'] = df['emotion1'].replace('none_1', 'no emotion')

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P10_self.csv', index=False)

from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt, savgol_filter
import scipy.signal as signal
from scipy.signal import welch
from scipy.signal import stft

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP4.csv'
self_annotation_file_path = '/content/Processed_P4_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(14, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548137961000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized4_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP5.csv'
self_annotation_file_path = '/content/Processed_P5_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(14, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548206905000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized5_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP8.csv'
self_annotation_file_path = '/content/Processed_P8_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(14, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548224621000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized8_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP9.csv'
self_annotation_file_path = '/content/Processed_P9_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(14, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548725858000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized9_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP10.csv'
self_annotation_file_path = '/content/Processed_P10_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(14, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548725861000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized10_data.csv', index=False)

"""# model"""

import pandas as pd

# List of file paths
file_paths = [
    '/content/sample_data/synchronized_data.csv',
    '/content/sample_data/synchronized4_data.csv',
    '/content/sample_data/synchronized5_data.csv',
    '/content/sample_data/synchronized8_data.csv',
    '/content/sample_data/synchronized9_data.csv',
    '/content/sample_data/synchronized10_data.csv',
    '/content/synchronized11_data.csv',
    '/content/synchronized12_data.csv',
    '/content/synchronized13_data.csv',
    '/content/synchronized14_data.csv',
    '/content/synchronized15_data.csv',
    '/content/synchronized16_data.csv',
    '/content/synchronized17_data.csv',
    '/content/synchronized19_data.csv',
    '/content/synchronized20_data.csv',
    '/content/synchronized21_data.csv',
    '/content/synchronized22_data.csv'
]

# Function to merge multiple CSV files into a single DataFrame
def merge_csv_files(file_paths):
    # Initialize an empty list to hold dataframes
    dataframes = []

    # Iterate over the file paths and read each CSV file
    for file_path in file_paths:
        df = pd.read_csv(file_path)
        dataframes.append(df)

    # Concatenate all dataframes
    merged_df = pd.concat(dataframes, ignore_index=True)

    return merged_df

# Merge the files
merged_df = merge_csv_files(file_paths)

# Save the merged dataframe to a new CSV file
merged_file_path = 'merged_synchronized_data.csv'
merged_df.to_csv(merged_file_path, index=False)

# Load the dataset
file_path = '/content/merged_synchronized_data.csv'
data = pd.read_csv(file_path)

from sklearn.model_selection import train_test_split
from sklearn.utils import resample

# Selecting the features and target
features = ['mean', 'std_dev', 'energy', 'arc_length', 'skewness', 'hf_psd', 'rhfpsd', 'stft_skewness', 'stft_kurtosis']
target = 'emotion'

# Split the data into features (X) and target (y)
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Check the distribution of target classes in the training set
class_distribution = y_train.value_counts()
print("Class distribution in training set before upsampling:\n", class_distribution)

# Upsample the minority classes in the training set
train_data = pd.concat([X_train, y_train], axis=1)

# Find the maximum class count
max_class_count = class_distribution.max()

# Upsample each class to the maximum class count
upsampled_data = []

for class_label, count in class_distribution.items():
    class_data = train_data[train_data[target] == class_label]
    if count < max_class_count:
        class_data_upsampled = resample(class_data,
                                        replace=True,  # sample with replacement
                                        n_samples=max_class_count,  # to match the majority class
                                        random_state=42)  # reproducible results
        upsampled_data.append(class_data_upsampled)
    else:
        upsampled_data.append(class_data)

# Combine the upsampled data
upsampled_train_data = pd.concat(upsampled_data)

# Check the new class distribution
new_class_distribution = upsampled_train_data[target].value_counts()
print("Class distribution in training set after upsampling:\n", new_class_distribution)

# Separate the features and target in the upsampled training set
X_train_upsampled = upsampled_train_data[features]
y_train_upsampled = upsampled_train_data[target]

# Display the new class distribution
new_class_distribution

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_selection import SelectFromModel

# Train a RandomForestClassifier to get feature importances
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_upsampled, y_train_upsampled)

# Get feature importances
importances = rf_classifier.feature_importances_
feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Select the most important features
selector = SelectFromModel(rf_classifier, threshold='mean', prefit=True)
selected_features = selector.get_support(indices=True)

# Transform the dataset to keep only the selected features
X_train_important = X_train_upsampled.iloc[:, selected_features]
X_test_important = X_test.iloc[:, selected_features]

# Encode the string labels to numerical labels
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train_upsampled)
y_test_encoded = label_encoder.transform(y_test)  # Similarly, encode y_test if needed for evaluation

# Train XGBoost classifier with selected features
xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')
xgb_classifier.fit(X_train_important, y_train_upsampled)

# Make predictions on the test set
y_pred = xgb_classifier.predict(X_test_important)

# Evaluate the model
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Display the confusion matrix
conf_matrix_df = pd.DataFrame(conf_matrix,
                              index=rf_classifier.classes_,
                              columns=rf_classifier.classes_)



# Display classification report
print("Classification Report with XGBoost:\n", class_report)

# Apply SMOTE to the training set
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Initialize the RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=42)

# Set up a very simplified parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2],
    'min_samples_leaf': [1],
    'bootstrap': [True]
}

# Initialize GridSearchCV with no parallel processing
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, n_jobs=1, verbose=2)

# Train the model with hyperparameter tuning
grid_search.fit(X_train_upsampled, y_train_upsampled)

# Get the best model from grid search
best_rf_classifier = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_rf_classifier.predict(X_test)

# Evaluate the model
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Display classification report
print("Classification Report:\n", class_report)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P11.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad'], inplace=True)

df.head()

df.to_csv('Processed_P11_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P12.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad'], inplace=True)

df.head()

df.to_csv('Processed_P12_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P13.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P13_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P14.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P14_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P15.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P15_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P16.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P16_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P17.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P17_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P18.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P18_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P19.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P19_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P20.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P20_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P21.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P21_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P22.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','delight','concentration','frustration','surprise','none_1'], inplace=True)

df.head()

df.to_csv('Processed_P22_self.csv', index=False)

from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt, savgol_filter

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP11.csv'
self_annotation_file_path = '/content/sample_data/Processed_P11_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Import necessary functions
from scipy.signal import welch, stft
import numpy as np

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548831934000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized11_data.csv', index=False)

from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt, savgol_filter

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP12.csv'
self_annotation_file_path = '/content/sample_data/Processed_P12_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(12, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548831937000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized12_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP13.csv'
self_annotation_file_path = '/content/sample_data/Processed_P13_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548898955000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized13_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP14.csv'
self_annotation_file_path = '/content/sample_data/Processed_P14_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1548898956000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized14_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP15.csv'
self_annotation_file_path = '/content/sample_data/Processed_P15_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1549935390000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized15_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP16.csv'
self_annotation_file_path = '/content/sample_data/Processed_P16_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1549935389000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized16_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP17.csv'
self_annotation_file_path = '/content/sample_data/Processed_P17_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552030184000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized17_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP19.csv'
self_annotation_file_path = '/content/sample_data/Processed_P19_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552117901000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized19_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP20.csv'
self_annotation_file_path = '/content/sample_data/Processed_P20_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Plot the raw signal
plt.figure(figsize=(15, 4))
plt.plot(bvp_data['value'], label='Raw Signal', alpha=0.5)
plt.title('Raw BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Plot the preprocessed signal
plt.figure(figsize=(15, 6))
plt.plot(bvp_data['smoothed_value'], label='Preprocessed Signal', alpha=0.75, color='orange')
plt.title('Preprocessed BVP Signal')
plt.xlabel('Time (samples)')
plt.ylabel('BVP Value')
plt.legend()
plt.show()

# Static Analysis
def static_analysis(data):
    stats = {
        'mean': np.mean(data),
        'std_dev': np.std(data),
        'min': np.min(data),
        'max': np.max(data),
        'skewness': skew(data),
        'kurtosis': kurtosis(data)
    }
    return stats

raw_stats = static_analysis(bvp_data['value'])
preprocessed_stats = static_analysis(bvp_data['smoothed_value'])

# Print the static analysis results
print("Static Analysis of Raw BVP Signal:")
for key, value in raw_stats.items():
    print(f"{key}: {value}")

print("\nStatic Analysis of Preprocessed BVP Signal:")
for key, value in preprocessed_stats.items():
    print(f"{key}: {value}")

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552117894000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized20_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP21.csv'
self_annotation_file_path = '/content/sample_data/Processed_P21_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552378718000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized21_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP22.csv'
self_annotation_file_path = '/content/sample_data/Processed_P22_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552378669000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()



synchronized_data.rename(columns={'pis': 'pid'}, inplace=True)

# Save or display the synchronized data
synchronized_data.to_csv('synchronized22_data.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P23.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P23_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P24.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P24_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P25.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head ()

df.to_csv('Processed_P25_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P26.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P25_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P26.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P26_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P27.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P27_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P28.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.head()

df.to_csv('Processed_P28_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P29.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P29_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P30.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P30_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P31.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P31_self.csv', index=False)

import pandas as pd

# Load the dataset
file_path = '/content/sample_data/P32.self.csv'
df = pd.read_csv(file_path)

# Step 1: Create column 'a-v' populated with the name of the column ('valence' or 'arousal') that has the maximum value
df['a-v'] = df[['valence', 'arousal']].idxmax(axis=1)

# Step 2: Create column 'emotion' with the appropriate emotion based on max value among specified columns
emotion_columns = ['happy', 'angry', 'nervous', 'sad', 'cheerful']
df['emotion'] = df[emotion_columns].idxmax(axis=1)

# Handle the case where all values are the same (all 1s in this context)
df['emotion'] = df.apply(lambda row: 'neutral' if len(set(row[emotion_columns])) == 1 else row['emotion'], axis=1)

df.drop(columns=['arousal','valence','cheerful','happy','angry','nervous','sad','boredom','confusion','concentration','none_1','delight','frustration','surprise'], inplace=True)

df.to_csv('Processed_P32_self.csv', index=False)

from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt, savgol_filter

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP23.csv'
self_annotation_file_path = '/content/Processed_P23_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552624550000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()



# Save or display the synchronized data
synchronized_data.to_csv('synchronized23_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP24.csv'
self_annotation_file_path = '/content/Processed_P24_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552624528000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized24_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP25.csv'
self_annotation_file_path = '/content/Processed_P25_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552633669000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized25_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP26.csv'
self_annotation_file_path = '/content/Processed_P26_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
from scipy.signal import welch, stft
from scipy.stats import skew
import numpy as np
import pandas as pd
from scipy.signal import butter, filtfilt
from scipy.signal import savgol_filter
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552633654000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized26_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP27.csv'
self_annotation_file_path = '/content/Processed_P27_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552895231000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized27_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP28.csv'
self_annotation_file_path = '/content/Processed_P28_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1552895259000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized28_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP29.csv'
self_annotation_file_path = '/content/Processed_P29_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1553835417000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized29_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP30.csv'
self_annotation_file_path = '/content/Processed_P30_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1553835415000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized_data30.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP31.csv'
self_annotation_file_path = '/content/Processed_P31_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1553844121000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized31_data.csv', index=False)

# Load the data
bvp_file_path = '/content/sample_data/E4_BVP32.csv'
self_annotation_file_path = '/content/Processed_P32_self.csv'

bvp_data = pd.read_csv(bvp_file_path)
self_annotation_data = pd.read_csv(self_annotation_file_path)

# Define the bandpass filter
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = filtfilt(b, a, data)
    return y

# Define the artifact removal function
def artifact_removal(data, threshold=3.0):
    mean = np.mean(data)
    std_dev = np.std(data)
    artifact_free_data = np.where(np.abs(data - mean) > threshold * std_dev, mean, data)
    return artifact_free_data

# Preprocess BVP data
fs = 64  # Sampling frequency
lowcut = 0.5  # Lower bound of the bandpass filter
highcut = 8.0  # Upper bound of the bandpass filter

# Apply bandpass filter
bvp_data['filtered_value'] = bandpass_filter(bvp_data['value'], lowcut, highcut, fs)

# Normalize the signal (Z-score normalization)
bvp_data['normalized_value'] = (bvp_data['filtered_value'] - np.mean(bvp_data['filtered_value'])) / np.std(bvp_data['filtered_value'])

# Apply artifact removal
bvp_data['artifact_removed_value'] = artifact_removal(bvp_data['normalized_value'])

# Apply smoothing using Savitzky-Golay filter
bvp_data['smoothed_value'] = savgol_filter(bvp_data['artifact_removed_value'], window_length=11, polyorder=3)

# Define time-domain feature extraction functions
def extract_time_domain_features(segment):
    features = {}
    features['mean'] = np.mean(segment)
    features['std_dev'] = np.std(segment)
    features['energy'] = np.sum(segment ** 2)
    features['arc_length'] = np.sum(np.abs(np.diff(segment)))
    features['skewness'] = skew(segment)
    return features

# Define frequency-domain feature extraction functions
def extract_frequency_domain_features(segment, fs):
    f, Pxx = welch(segment, fs=fs, nperseg=256)

    hf_band = (0.15, 0.4)
    lf_band = (0.04, 0.15)

    hf_indices = np.logical_and(f >= hf_band[0], f <= hf_band[1])
    lf_indices = np.logical_and(f >= lf_band[0], f <= lf_band[1])

    hf_psd = np.sum(Pxx[hf_indices])
    lf_psd = np.sum(Pxx[lf_indices])

    total_psd = np.sum(Pxx)

    rhfpsd = hf_psd / total_psd
    rlfpsd = lf_psd / total_psd

    features = {
        'hf_psd': hf_psd,
        'rhfpsd': rhfpsd,
        'lf_psd': lf_psd,
        'rlfpsd': rlfpsd
    }

    return features

# Define additional feature extraction functions (STFT skewness, kurtosis)
def extract_additional_features(segment, fs):
    f, t, Zxx = stft(segment, fs=fs, nperseg=128)
    stft_magnitude = np.abs(Zxx)

    stft_skewness = skew(stft_magnitude.flatten())
    stft_kurtosis = np.mean((stft_magnitude.flatten() - np.mean(stft_magnitude.flatten()))**4) / np.var(stft_magnitude.flatten())**2

    features = {
        'stft_skewness': stft_skewness,
        'stft_kurtosis': stft_kurtosis
    }

    return features

# Integrate all feature extraction functions
def extract_all_features(bvp_data, fs, window_size=5):
    n_samples = int(window_size * fs)
    n_windows = len(bvp_data) // n_samples

    all_features = []

    for i in range(n_windows):
        segment = bvp_data['smoothed_value'].iloc[i*n_samples:(i+1)*n_samples]
        time_features = extract_time_domain_features(segment)
        freq_features = extract_frequency_domain_features(segment, fs)
        additional_features = extract_additional_features(segment, fs)

        features = {**time_features, **freq_features, **additional_features}
        all_features.append(features)

    return pd.DataFrame(all_features)

# Extract all features
all_features = extract_all_features(bvp_data, fs)

# Synchronize with self-annotation data
bvp_data['timestamp'] = pd.to_datetime(bvp_data['timestamp'], unit='ms')
start_time = pd.to_datetime(1553844125000, unit='ms')
intervals = pd.date_range(start=start_time, periods=len(all_features), freq='5S')
all_features['interval_start'] = intervals

self_annotation_data['interval_start'] = start_time + pd.to_timedelta(self_annotation_data['seconds'], unit='s')
synchronized_data = pd.merge_asof(all_features, self_annotation_data, on='interval_start', direction='nearest')

synchronized_data.head()

# Save or display the synchronized data
synchronized_data.to_csv('synchronized32_data.csv', index=False)