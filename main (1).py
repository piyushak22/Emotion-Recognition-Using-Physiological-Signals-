# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OqIyedZb3ccmzjyAX9NcIKOlibaYZLNI
"""

import pandas as pd

# List of file paths
file_paths = ['/content/sample_data/synchronized1_bvp_features_final.csv',
              '/content/sample_data/synchronized4_bvp_features_final.csv',
              '/content/sample_data/synchronized5_bvp_features_final.csv',
              '/content/sample_data/synchronized8_bvp_features_final.csv',
              '/content/sample_data/synchronized9_bvp_features_final.csv',
              '/content/sample_data/synchronized10_bvp_features_final.csv',
              '/content/sample_data/synchronized11_bvp_features_final.csv',
              '/content/sample_data/synchronized12_bvp_features_final.csv',
              '/content/sample_data/synchronized13_bvp_features_final.csv',
              '/content/sample_data/synchronized14_bvp_features_final.csv',
              '/content/sample_data/synchronized15_bvp_features_final.csv',
              '/content/sample_data/synchronized16_bvp_features_final.csv',
              '/content/sample_data/synchronized17_bvp_features_final.csv',
              '/content/sample_data/synchronized19_bvp_features_final.csv',
              '/content/sample_data/synchronized20_bvp_features_final.csv',
              '/content/sample_data/synchronized21_bvp_features_final.csv',
              '/content/sample_data/synchronized22_bvp_features_final.csv',
              '/content/sample_data/synchronized23_bvp_features_final.csv',
              '/content/sample_data/synchronized24_bvp_features_final.csv',
              '/content/sample_data/synchronized25_bvp_features_final.csv',
              '/content/sample_data/synchronized26_bvp_features_final.csv',
              '/content/sample_data/synchronized27_bvp_features_final.csv',
              '/content/sample_data/synchronized28_bvp_features_final.csv',
              '/content/sample_data/synchronized29_bvp_features_final.csv',
              '/content/sample_data/synchronized30_bvp_features_final.csv',
              '/content/sample_data/synchronized31_bvp_features_final.csv',

]

# Function to merge multiple CSV files into a single DataFrame
def merge_csv_files(file_paths):
    # Initialize an empty list to hold dataframes
    dataframes = []

    # Iterate over the file paths and read each CSV file
    for file_path in file_paths:
        df = pd.read_csv(file_path)
        dataframes.append(df)

    # Concatenate all dataframes
    merged_df = pd.concat(dataframes, ignore_index=True)

    return merged_df

# Merge the files
merged_df = merge_csv_files(file_paths)

# Remove empty rows
df_cleaned = merged_df.dropna(how='all')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.feature_selection import RFE
from sklearn.utils import resample

# Load the dataset
file_path = '/content/sample_data/merged_synchronized2_data (1).csv'
df = pd.read_csv(file_path)

# Calculate the mean only for numeric columns
numeric_columns = df.select_dtypes(include=['number']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Select only the specified columns as features
feature_columns = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD',
    'SampEn', 'Number_of_SCRs', 'Summed_Magnitude_of_SCRs',
    'Summed_Area_Under_SCRs', 'Mean_Tonic_Level', 'Std_Tonic_Level',
    'Mean_Phasic_Activity', 'Number_of_Responses', 'Skewness_y', 'Kurtosis_y', 'AVNN', 'CV', 'SDNN', 'RMSSD', 'SDSD'
]

X = df[feature_columns]  # Features
y = df['a-v']  # Target (valence/arousal)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Combine the training data back into a dataframe for upsampling
train_df = pd.concat([X_train, y_train], axis=1)

classes = [train_df[train_df['a-v'] == label] for label in y_train.value_counts().index]
max_count = y_train.value_counts().max()

classes_upsampled = [
    resample(cls, replace=True, n_samples=max_count, random_state=42) if len(cls) < max_count else cls
    for cls in classes
]

# Combine the upsampled classes into a single dataframe
train_df_upsampled = pd.concat(classes_upsampled)

# Shuffle the resulting dataframe
train_df_upsampled = train_df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)

# Check the distribution of the 'emotion' column after upsampling
upsampled_class_counts = train_df_upsampled['a-v'].value_counts()
upsampled_class_counts

# Separate the features and the target variable again
X_train_upsampled = train_df_upsampled[feature_columns]
y_train_upsampled = train_df_upsampled['a-v']

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_upsampled)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Classifier to get feature importance
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_scaled, y_train_upsampled)

# Make predictions on the test set
y_pred = clf.predict(X_test_scaled)

# Evaluate the model's performance
report = classification_report(y_test, y_pred)
print(report)

# Get feature importances
feature_importances = clf.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance from Random Forest')
plt.gca().invert_yaxis()
plt.show()

# Select top features based on importance
top_features = feature_importance_df['Feature'][:10].tolist()
X_train_top = X_train_scaled[:, :len(top_features)]
X_test_top = X_test_scaled[:, :len(top_features)]

from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define the parameter grid
param_dist = {
    'n_estimators': np.arange(50, 300, 50),
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': np.arange(3, 8),
    'min_samples_split': [2, 5, 10]
}

# Initialize the model
gbc = GradientBoostingClassifier(random_state=42)

# Use RandomizedSearchCV instead of GridSearchCV
random_search = RandomizedSearchCV(gbc, param_distributions=param_dist, n_iter=20, cv=3, scoring='f1_weighted', n_jobs=-1, random_state=42, verbose=2)
random_search.fit(X_train_top, y_train_upsampled)

# Get the best parameters and the best estimator
best_params = random_search.best_params_
best_gbc = random_search.best_estimator_

# Make predictions on the test set
y_pred = best_gbc.predict(X_test_top)

# Evaluate the model's performance
report = classification_report(y_test, y_pred)
print(report)

"""# **BVP+EDA with RF , XGBOOST , MLP** **EMOTION**"""

import pandas as pd
import numpy as np
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import compute_class_weight # Import the missing function

# Load the dataset
file_path = '/content/sample_data/merged_synchronized2_data (1).csv'
df = pd.read_csv(file_path)

# Replace missing values in each column with the mean of that specific column
numeric_columns = df.select_dtypes(include=['number']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Filter out the "sad" class from the dataset
df_filtered = df[df['emotion'] != 'sad']

# Select the features and the target
feature_columns = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD',
    'SampEn', 'Number_of_SCRs', 'Summed_Magnitude_of_SCRs',
    'Summed_Area_Under_SCRs', 'Mean_Tonic_Level', 'Std_Tonic_Level',
    'Mean_Phasic_Activity', 'Number_of_Responses', 'Skewness_y', 'Kurtosis_y'
]

X = df_filtered[feature_columns]  # Features
y = df_filtered['emotion']  # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Compute the correlation matrix
corr_matrix = X_train.corr()

# Identify features to drop due to high correlation
threshold = 0.9
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]

print(f"Features to drop due to high correlation: {to_drop}")

# Drop the highly correlated features from X_train and X_test
X_train_dropped = X_train.drop(columns=to_drop)
X_test_dropped = X_test.drop(columns=to_drop)

# Combine the training data back into a dataframe for upsampling
train_df = pd.concat([X_train, y_train], axis=1)

# Upsample the minority classes in the training set
classes = [train_df[train_df['emotion'] == label] for label in y_train.value_counts().index]
max_count = y_train.value_counts().max()

classes_upsampled = [
    resample(cls, replace=True, n_samples=max_count, random_state=42) if len(cls) < max_count else cls
    for cls in classes
]

# Combine the upsampled classes into a single dataframe
train_df_upsampled = pd.concat(classes_upsampled)

# Shuffle the resulting dataframe
train_df_upsampled = train_df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)

# Check the distribution of the 'emotion' column after upsampling
upsampled_class_counts = train_df_upsampled['emotion'].value_counts()
upsampled_class_counts

# Separate features and target from upsampled data
X_train_upsampled = train_df_upsampled[feature_columns]
y_train_upsampled = train_df_upsampled['emotion']

# Encode the target variable
le = LabelEncoder()
y_train_upsampled_encoded = le.fit_transform(y_train_upsampled)
y_test_encoded = le.transform(y_test)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_upsampled)
X_test_scaled = scaler.transform(X_test)  # Assuming X_test is already defined

# Train a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train_upsampled_encoded)

# Predict on the test set
y_pred = rf.predict(X_test_scaled)

# Evaluate the model's performance
print(classification_report(y_test_encoded, y_pred))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
import matplotlib.pyplot as plt

# Generate the confusion matrix
rf_cm = confusion_matrix(y_test_encoded, y_pred)

# Display the confusion matrix
ConfusionMatrixDisplay(rf_cm, display_labels=le.classes_).plot()

# Show the plot
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder

# Initialize and fit LabelEncoder (assuming 'y' is your target variable)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Replace 'y' with your actual target variable

model = Sequential([
    Input(shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer with softmax activation
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the MLP model
model.fit(X_train_scaled, y_train_upsampled_encoded, epochs=50, batch_size=32, validation_split=0.2, verbose=2)

# Predict on the test set using MLP
y_pred_probs = model.predict(X_test_scaled)
y_pred = y_pred_probs.argmax(axis=1)

# Evaluate the MLP model's performance
from sklearn.metrics import classification_report, confusion_matrix
print("Simplified MLP Model Performance")
print(classification_report(y_test_encoded, y_pred))
print(confusion_matrix(y_test_encoded, y_pred))

# Calculate the confusion matrix
mlp_cm = confusion_matrix(y_test_encoded, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(mlp_cm)

# Display the confusion matrix using a heatmap
disp = ConfusionMatrixDisplay(confusion_matrix=mlp_cm, display_labels=le.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for MLP Model")
plt.show()

# Print the overall accuracy
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f"Overall Accuracy: {accuracy:.2f}")

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Define and train the XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train_upsampled_encoded)

# Predict on the test set using XGBoost
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Print the classification report for XGBoost model
print("XGBoost Model Performance:")
print(classification_report(y_test_encoded, y_pred_xgb))

# Calculate the confusion matrix for XGBoost model
xgb_cm = confusion_matrix(y_test_encoded, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(xgb_cm)

# Display the confusion matrix using a heatmap
disp = ConfusionMatrixDisplay(confusion_matrix=xgb_cm, display_labels=le.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for XGBoost Model")
plt.show()

"""# **BVP+EDA with A-V**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import resample
from sklearn.utils import compute_class_weight
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Replace missing values in each column with the mean of that specific column
numeric_columns = df.select_dtypes(include=['number']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Select the features and the target
feature_columns = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD',
    'SampEn', 'Number_of_SCRs', 'Summed_Magnitude_of_SCRs',
    'Summed_Area_Under_SCRs', 'Mean_Tonic_Level', 'Std_Tonic_Level',
    'Mean_Phasic_Activity', 'Number_of_Responses', 'Skewness_y', 'Kurtosis_y'
]

X = df[feature_columns]  # Features
y = df['a-v']  # Target (using 'a-v' column)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Combine the training data to address class imbalance through upsampling
train_data = pd.concat([X_train, y_train], axis=1)

# Upsample the minority class
majority_class = train_data['a-v'].value_counts().idxmax()
upsampled_train_data = []

for label in y_train.value_counts().index:
    label_data = train_data[train_data['a-v'] == label]
    upsampled_label_data = resample(label_data, replace=True, n_samples=y_train.value_counts().max(), random_state=42)
    upsampled_train_data.append(upsampled_label_data)

# Combine the upsampled data
train_data_upsampled = pd.concat(upsampled_train_data)

# Separate the features and target from the upsampled data
X_train_upsampled = train_data_upsampled[feature_columns]
y_train_upsampled = train_data_upsampled['a-v']

upsampled_class_counts = y_train_upsampled.value_counts()

# Display the distribution
print("Class distribution after upsampling:")
print(upsampled_class_counts)

# Visualize the class distribution using a bar plot
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
upsampled_class_counts.plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Class Distribution After Upsampling')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_upsampled)
X_test_scaled = scaler.transform(X_test)

# Label encoding for the target labels (0 and 1)
label_encoder = LabelEncoder()
y_train_upsampled_encoded = label_encoder.fit_transform(y_train_upsampled)
y_test_encoded = label_encoder.transform(y_test)

"""# ----------- Simplified MLP Model -----------"""

# Compute initial class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_upsampled_encoded), y=y_train_upsampled_encoded)
class_weights = dict(enumerate(class_weights))

# Adjust class weights more conservatively
class_weights[0] = 1.0
class_weights[1] = 1.0

# ----------- Define and Train the MLP Model with Adjusted Class Weights -----------
model = Sequential([
    Input(shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])

# Train the MLP model with more balanced class weights
history = model.fit(X_train_scaled, y_train_upsampled_encoded, epochs=50, batch_size=32, validation_split=0.2, verbose=2, class_weight=class_weights)

# Predict on the test set
y_pred_probs = model.predict(X_test_scaled)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()

# Print the classification report for MLP model
print("MLP Model Performance with Balanced Class Weights:")
print(classification_report(y_test_encoded, y_pred))

# Calculate the confusion matrix for MLP model
mlp_cm = confusion_matrix(y_test_encoded, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(mlp_cm)

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming y_test_encoded and y_pred are already defined
labels_to_consider = np.unique(y_test_encoded)

# Generate the confusion matrix
cm = confusion_matrix(y_test_encoded, y_pred, labels=labels_to_consider)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_to_consider)
disp.plot(cmap=plt.cm.Blues)
plt.show()

labels_to_consider = np.unique(y_test_encoded)
confusion_matrix(y_test_encoded, y_pred, labels=labels_to_consider)

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Calculate scale_pos_weight (ratio of negative to positive samples)
neg_samples = sum(y_train_upsampled_encoded == 0)
pos_samples = sum(y_train_upsampled_encoded == 1)

# If there are multiple classes, you can calculate this for the majority and minority classes
scale_pos_weight = neg_samples / pos_samples

# Define the parameter grid for GridSearchCV
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300],
    'scale_pos_weight': [1, scale_pos_weight]

# Initialize XGBoost without use_label_encoder
xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)

# Fit the model to the training data
xgb_model.fit(X_train_scaled, y_train_upsampled_encoded)

# Predict on the test set
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='f1', verbose=2, n_jobs=1)

# Fit the model
grid_search.fit(X_train_scaled, y_train_upsampled_encoded)

# Get the best parameters from the grid search
best_params = grid_search.best_params_
print(f"Best parameters found: {best_params}")

# Evaluate the best model on the test set
best_xgb_model = grid_search.best_estimator_
y_pred_xgb = best_xgb_model.predict(X_test_scaled)

# Print performance metrics
print("XGBoost Model Performance after Tuning")
print(classification_report(y_test_encoded, y_pred_xgb))
print(confusion_matrix(y_test_encoded, y_pred_xgb))

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Print performance metrics
print("XGBoost Model Performance after Tuning")
print(classification_report(y_test_encoded, y_pred_xgb))

# Generate the confusion matrix
xgb_cm = confusion_matrix(y_test_encoded, y_pred_xgb)

# Display the confusion matrix
ConfusionMatrixDisplay(xgb_cm, display_labels=le.classes_).plot()

# Show the plot
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid for RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False],
    'class_weight': ['balanced', 'balanced_subsample', None]
}

# Initialize the RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search_rf = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_grid_rf,
    n_iter=50,  # Number of random combinations to try
    cv=5,  # 5-fold cross-validation
    verbose=2,
    random_state=42,
    n_jobs=1
)

# Fit the model
random_search_rf.fit(X_train_scaled, y_train_upsampled_encoded)

# Predict on the test set
y_pred = random_search_rf.predict(X_test_scaled)

# Evaluate the model's performance
print(classification_report(y_test_encoded, y_pred))

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict on the test set
y_pred = random_search_rf.predict(X_test_scaled)

# Evaluate the model's performance
print("Random Forest Model Performance after RandomizedSearchCV Tuning")
print(classification_report(y_test_encoded, y_pred))

# Generate and display the confusion matrix
rf_cm = confusion_matrix(y_test_encoded, y_pred)
ConfusionMatrixDisplay(rf_cm, display_labels=le.classes_).plot()

# Show the plot
plt.show()

"""# **BVP+EDA+HR-RATE with emotion **

# random forest and xgboost
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import RandomizedSearchCV
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/content/sample_data/merged_synchronized2_data (1) (1).csv'
df = pd.read_csv(file_path)

# Print the distribution of classes in the 'emotion' column before dropping values
class_distribution_before_dropping = df['emotion'].value_counts()

# Print the distribution
print("Class distribution in the 'emotion' column before dropping values:")
print(class_distribution_before_dropping)

# Ensure datetime conversion for timestamp_x column
df['timestamp_x'] = pd.to_datetime(df['timestamp_x'], format='%d/%m/%Y %H:%M')

# Extract time-based features from timestamp_x
df['hour'] = df['timestamp_x'].dt.hour
df['minute'] = df['timestamp_x'].dt.minute
df['second'] = df['timestamp_x'].dt.second

# Calculate elapsed time from the start for each participant using timestamp_x
df['elapsed_time'] = df.groupby('pid')['seconds'].transform(lambda x: x - x.min())

# Creating lag features for key physiological signals
df['Mean_lag_1'] = df.groupby('pid')['Mean'].shift(1)
df['Std_Dev_lag_1'] = df.groupby('pid')['Std_Dev'].shift(1)
df['Arc_Length_lag_1'] = df.groupby('pid')['Arc_Length'].shift(1)

# Fill NaN values in lag features with the corresponding original feature value
df['Mean_lag_1'].fillna(df['Mean'], inplace=True)
df['Std_Dev_lag_1'].fillna(df['Std_Dev'], inplace=True)
df['Arc_Length_lag_1'].fillna(df['Arc_Length'], inplace=True)

# Replace empty (NaN) values in numeric columns with the mean of their respective columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Print the distribution of classes in the 'emotion' column before dropping values
class_distribution_before_dropping = df['emotion'].value_counts()

# Print the distribution
print("Class distribution in the 'emotion' column before dropping values:")
print(class_distribution_before_dropping)

# Remove classes with only one sample in the target variable
class_counts = df['emotion'].value_counts()
classes_to_keep = class_counts[class_counts > 1].index
df = df[df['emotion'].isin(classes_to_keep)]

# Define the features before and after feature engineering
features = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD', 'SampEn',
    'Number_of_SCRs', 'Summed_Magnitude_of_SCRs', 'Summed_Area_Under_SCRs',
    'Mean_Tonic_Level', 'Std_Tonic_Level', 'Mean_Phasic_Activity', 'Number_of_Responses',
    'Skewness_y', 'Kurtosis_y', 'AVNN', 'CV', 'SDNN', 'RMSSD', 'SDSD',
    'Mean_lag_1', 'Std_Dev_lag_1', 'Arc_Length_lag_1'
]
target = 'emotion'

# Check for constant features
constant_features = [col for col in X_train.columns if X_train[col].nunique() == 1]
print(f"Constant features: {constant_features}")

# Calculate the correlation matrix
corr_matrix = df[features].corr()

# Plot the correlation matrix
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})
plt.title("Correlation Matrix")
plt.show()

# Removing highly correlated features
threshold = 0.75
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]

# Dropping the features
df_reduced = df.drop(columns=to_drop)

# Update the features list to only include those that remain
features_reduced = [feature for feature in features if feature not in to_drop]

# Recalculate the correlation matrix after removal
reduced_corr_matrix = df_reduced[features_reduced].corr()

# Plot the reduced correlation matrix
plt.figure(figsize=(14, 10))
sns.heatmap(reduced_corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})
plt.title("Correlation Matrix After Removing Highly Correlated Features")
plt.show()

# Optional: Calculate VIF to further check multicollinearity
X = sm.add_constant(df_reduced[features_reduced])
vif = pd.DataFrame()
vif["Features"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF values
print("Variance Inflation Factor (VIF):")
print(vif)

import matplotlib.pyplot as plt
import pandas as pd

# Assuming you have your VIF data in a DataFrame
vif_data = pd.DataFrame({
    'Features': ['const', 'Mean', 'Std_Dev', 'Skewness_x', 'Kurtosis_x', 'SampEn',
                 'Number_of_SCRs', 'Summed_Magnitude_of_SCRs', 'Std_Tonic_Level',
                 'Mean_Phasic_Activity', 'Number_of_Responses', 'Skewness_y',
                 'Kurtosis_y', 'AVNN', 'CV', 'SDNN', 'Mean_lag_1', 'Std_Dev_lag_1'],
    'VIF': [53.508096, 1.645217, 2.607476, 1.111661, 1.343254, 1.493133,
            1.684089, 1.391005, 1.463104, 1.546378, 1.562543, 1.204158,
            1.560208, 1.164117, 1.375805, 1.296945, 1.608214, 2.230607]
})

# Create bar plot
plt.figure(figsize=(10, 8))
plt.barh(vif_data['Features'], vif_data['VIF'], color='skyblue')
plt.xlabel('VIF Value')
plt.title('Variance Inflation Factor (VIF) for Each Feature')
plt.axvline(x=10, color='red', linestyle='--')  # Adding a line to indicate VIF = 10 threshold
plt.show()

# Split the data into training and testing sets
X = df_reduced[features_reduced]
y = df_reduced[target]

# Remove the "sad" class
df_filtered = df[df['emotion'] != 'sad']

# Update features and target
X_filtered = df_filtered[features]
y_filtered = df_filtered['emotion']

# Encode the target labels as integers
label_encoder = LabelEncoder()
y_filtered_encoded = label_encoder.fit_transform(y_filtered)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered_encoded, test_size=0.2, random_state=42, stratify=y_filtered_encoded)

# Perform SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check the distribution of classes after SMOTE
print("Class distribution after SMOTE:")
print(pd.Series(y_train_smote).value_counts())

# Visualize the class distribution after SMOTE
plt.figure(figsize=(8, 6))
pd.Series(y_train_smote).value_counts().plot(kind='bar', color='skyblue')
plt.title('Class Distribution After SMOTE')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# Set up the XGBoost classifier with hyperparameter tuning
xgb = XGBClassifier(random_state=42)

param_dist = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist,
                                   n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=1)

random_search.fit(X_train_scaled, y_train_smote)

# Make predictions on the test set
y_pred = random_search.predict(X_test_scaled)

# Decode the predicted labels back to the original labels
y_pred_decoded = label_encoder.inverse_transform(y_pred)
y_test_decoded = label_encoder.inverse_transform(y_test)

# Evaluate the model
accuracy = accuracy_score(y_test_decoded, y_pred_decoded)
report = classification_report(y_test_decoded, y_pred_decoded)

# Print the results
print(f'Best Parameters: {random_search.best_params_}')
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)

# Generate the confusion matrix
cm = confusion_matrix(y_test_decoded, y_pred_decoded)

# Display the confusion matrix
ConfusionMatrixDisplay(cm, display_labels=label_encoder.classes_).plot()

# Show the plot
plt.show()

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Set up RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
random_search_rf = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),
                                      param_distributions=param_dist,
                                      n_iter=20, cv=3, scoring='accuracy',
                                      random_state=42, n_jobs=1)

# Fit the model
random_search_rf.fit(X_train_scaled, y_train_smote)

# Make predictions on the test set
y_pred_rf = random_search_rf.predict(X_test_scaled)

# Decode the predicted labels back to the original labels
y_pred_rf_decoded = label_encoder.inverse_transform(y_pred_rf)
y_test_decoded = label_encoder.inverse_transform(y_test)

# Now evaluate the model
accuracy_rf = accuracy_score(y_test_decoded, y_pred_rf_decoded)
report_rf = classification_report(y_test_decoded, y_pred_rf_decoded)

# Print the results
print(f'Best Parameters for Random Forest: {random_search_rf.best_params_}')
print(f'Accuracy with Random Forest: {accuracy_rf}')
print('Classification Report for Random Forest:')
print(report_rf)

importances = random_search_rf.best_estimator_.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(X_train_scaled.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train_scaled.shape[1]), [X_train.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X_train_scaled.shape[1]])
plt.show()

from sklearn.model_selection import learning_curve

train_sizes, train_scores, valid_scores = learning_curve(
    random_search_rf.best_estimator_, X_train_scaled, y_train_smote, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 5), scoring='accuracy')

train_scores_mean = np.mean(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)

plt.figure()
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.legend(loc="best")
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
ConfusionMatrixDisplay(cm_normalized, display_labels=label_encoder.classes_).plot(cmap=plt.cm.Blues)
plt.title('Normalized Confusion Matrix')
plt.show()

import seaborn as sns
report = classification_report(y_test_decoded, y_pred_rf_decoded, output_dict=True)
sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)
plt.title('Classification Report Heatmap')
plt.show()

# Generate the confusion matrix
cm = confusion_matrix(y_test_decoded, y_pred_rf_decoded)

# Display the confusion matrix
ConfusionMatrixDisplay(cm, display_labels=label_encoder.classes_).plot()

# Show the plot
plt.show()

"""# **BVP+EDA+HR a-v**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Print the distribution of classes in the 'a-v' column before dropping values
class_distribution_before_dropping = df['a-v'].value_counts()

# Print the distribution
print("Class distribution in the 'emotion' column before dropping values:")
print(class_distribution_before_dropping)

# Ensure datetime conversion for timestamp_x column
df['timestamp_x'] = pd.to_datetime(df['timestamp_x'], format='%d/%m/%Y %H:%M')

# Extract time-based features from timestamp_x
df['hour'] = df['timestamp_x'].dt.hour
df['minute'] = df['timestamp_x'].dt.minute
df['second'] = df['timestamp_x'].dt.second

# Calculate elapsed time from the start for each participant using timestamp_x
df['elapsed_time'] = df.groupby('pid')['seconds'].transform(lambda x: x - x.min())

# Creating lag features for key physiological signals
df['Mean_lag_1'] = df.groupby('pid')['Mean'].shift(1)
df['Std_Dev_lag_1'] = df.groupby('pid')['Std_Dev'].shift(1)
df['Arc_Length_lag_1'] = df.groupby('pid')['Arc_Length'].shift(1)

# Fill NaN values in lag features with the corresponding original feature value
df['Mean_lag_1'].fillna(df['Mean'], inplace=True)
df['Std_Dev_lag_1'].fillna(df['Std_Dev'], inplace=True)
df['Arc_Length_lag_1'].fillna(df['Arc_Length'], inplace=True)

# Replace empty (NaN) values in numeric columns with the mean of their respective columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Print the distribution of classes in the 'emotion' column before dropping values
class_distribution_before_dropping = df['a-v'].value_counts()

# Print the distribution
print("Class distribution in the 'a-v' column before dropping values:")
print(class_distribution_before_dropping)

# Define initial features and target
features = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD', 'SampEn',
    'Number_of_SCRs', 'Summed_Magnitude_of_SCRs', 'Summed_Area_Under_SCRs',
    'Mean_Tonic_Level', 'Std_Tonic_Level', 'Mean_Phasic_Activity', 'Number_of_Responses',
    'Skewness_y', 'Kurtosis_y', 'AVNN', 'CV', 'SDNN', 'RMSSD', 'SDSD',
    'Mean_lag_1', 'Std_Dev_lag_1', 'Arc_Length_lag_1'
]
target = 'a-v'

# Calculate the correlation matrix
corr_matrix = df[features].corr()

# Removing highly correlated features
threshold = 0.75
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]

# Dropping the features from the original dataframe
df_reduced = df.drop(columns=to_drop)

# Update the features list to only include those that remain
features_reduced = [feature for feature in features if feature not in to_drop]

# Recalculate the correlation matrix after removal
reduced_corr_matrix = df_reduced[features_reduced].corr()

# Plot the reduced correlation matrix
plt.figure(figsize=(14, 10))
sns.heatmap(reduced_corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})
plt.title("Correlation Matrix After Removing Highly Correlated Features")
plt.show()

# Split the data into training and testing sets
X = df_reduced[features_reduced]
y = df_reduced[target]

# Encode the target labels as integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Apply SMOTE to balance the classes in the training data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check the distribution of classes after SMOTE
print("Class distribution after SMOTE:")
print(pd.Series(y_train_smote).value_counts())

# Visualize the class distribution after SMOTE
plt.figure(figsize=(8, 6))
pd.Series(y_train_smote).value_counts().plot(kind='bar', color='skyblue')
plt.title('a-v Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# Define the parameter grid for RandomizedSearchCV
param_dist_rf = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

random_search_rf = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),
                                      param_distributions=param_dist,
                                      n_iter=20, cv=3, scoring='accuracy',
                                      random_state=42, n_jobs=1)

# Fit the model
random_search_rf.fit(X_train_scaled, y_train_smote)

# Make predictions on the test set
y_pred_rf = random_search_rf.predict(X_test_scaled)

# Decode the predicted labels back to the original labels
y_pred_rf_decoded = label_encoder.inverse_transform(y_pred_rf)
y_test_decoded = label_encoder.inverse_transform(y_test)

# Evaluate the model's performance
accuracy_rf = classification_report(y_test_decoded, y_pred_rf_decoded)
print("Random Forest with SMOTE Performance")
print(accuracy_rf)

import seaborn as sns
report = classification_report(y_test_decoded, y_pred_rf_decoded, output_dict=True)
sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)
plt.title('Classification Report Heatmap')
plt.show()

# Generate the confusion matrix
cm_rf = confusion_matrix(y_test_decoded, y_pred_rf_decoded)

# Display the confusion matrix
ConfusionMatrixDisplay(cm_rf, display_labels=label_encoder.classes_).plot()

# Show the plot
plt.show()

# Feature Importance
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
selected_features = [features_reduced[i] for i in indices]

# Plotting Feature Importance
plt.figure(figsize=(12, 8))
plt.title("Feature Importance")
plt.bar(range(len(features_reduced)), importances[indices], align="center")
plt.xticks(range(len(features_reduced)), selected_features, rotation=90)
plt.tight_layout()
plt.show()

"""# **XGBOOST**"""

# Train an XGBoost model
xgb_model = XGBClassifier(random_state=42)

# Define the parameter grid for hyperparameter tuning
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist,
                                   n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=1)
random_search.fit(X_train_scaled, y_train_smote)

# Best parameters found by RandomizedSearchCV
print(f"Best Parameters: {random_search.best_params_}")

# Make predictions on the test set
y_pred = random_search.predict(X_test_scaled)

# Decode the predicted labels back to the original labels
y_pred_decoded = label_encoder.inverse_transform(y_pred)
y_test_decoded = label_encoder.inverse_transform(y_test)

# Evaluate the model
accuracy = classification_report(y_test_decoded, y_pred_decoded)
print(f'Accuracy: {accuracy}')

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_decoded, y_pred_decoded)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization , Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf # Import tensorflow

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_smote), y=y_train_smote)
class_weights_dict = dict(enumerate(class_weights))

from tensorflow.keras.losses import BinaryCrossentropy

def focal_loss(gamma=2., alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        bce = BinaryCrossentropy()(y_true, y_pred)
        y_true = tf.cast(y_true, dtype=tf.float32)
        alpha_t = y_true * alpha + (tf.ones_like(y_true) - y_true) * (1 - alpha)
        p_t = y_true * y_pred + (tf.ones_like(y_true) - y_true) * (tf.ones_like(y_pred) - y_pred)
        fl = alpha_t * tf.pow((tf.ones_like(y_true) - p_t), gamma) * bce
        return fl
    return focal_loss_fixed

def create_model():
    model = Sequential([
        Input(shape=(X_train_scaled.shape[1],)),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(1, activation='sigmoid')  # For binary classification
    ])

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=0.0005),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model

model = create_model()

# Compile the model with a smaller learning rate and Focal Loss
model.compile(optimizer=Adam(learning_rate=0.0005), loss=focal_loss(), metrics=['accuracy'])

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

# Train the model
history = model.fit(X_train_scaled, y_train_smote,
                    epochs=150,
                    batch_size=32,
                    validation_split=0.2,
                    class_weight=class_weights_dict,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)],
                    verbose=2)

# Predicting and evaluating the model
y_pred_probs = model.predict(X_test_scaled)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()

# Decode the predicted labels back to the original labels
y_pred_decoded = label_encoder.inverse_transform(y_pred)
y_test_decoded = label_encoder.inverse_transform(y_test)

# Classification Report
print("ANN with SMOTE Performance")
print(classification_report(y_test_decoded, y_pred_decoded))

def create_simpler_model(learning_rate=0.001, dropout_rate=0.4, layer_size=64):
    model = Sequential()
    model.add(Input(shape=(X_train_scaled.shape[1],)))
    model.add(Dense(layer_size, activation='relu'))
    model.add(Dropout(dropout_rate))

    model.add(Dense(layer_size // 2, activation='relu'))
    model.add(Dropout(dropout_rate))

    model.add(Dense(1, activation='sigmoid'))

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Train the simpler model
simpler_model = create_simpler_model()
history = simpler_model.fit(X_train_scaled, y_train_smote, epochs=50, batch_size=16, validation_split=0.2, verbose=2)

# Evaluate the model's performance on the test set
y_pred_probs = simpler_model.predict(X_test_scaled)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()

# Generate the classification report
report = classification_report(y_test, y_pred)
print("Simpler ANN Model Performance")
print(report)

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""# **LOSO** validation model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import LeaveOneGroupOut, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from collections import Counter

# Load the dataset
file_path = '/content/sample_data/merged_synchronized2_data (1) (1).csv'
df = pd.read_csv(file_path)

# Exclude participants 2, 3, 6, 7, 18
excluded_pids = [2, 3, 6, 7, 18]
df = df[~df['pid'].isin(excluded_pids)]

# Ensure datetime conversion for timestamp_x column
df['timestamp_x'] = pd.to_datetime(df['timestamp_x'], format='%d/%m/%Y %H:%M')

# Extract time-based features from timestamp_x
df['hour'] = df['timestamp_x'].dt.hour
df['minute'] = df['timestamp_x'].dt.minute
df['second'] = df['timestamp_x'].dt.second

# Calculate elapsed time from the start for each participant using timestamp_x
df['elapsed_time'] = df.groupby('pid')['seconds'].transform(lambda x: x - x.min())

# Creating lag features for key physiological signals
df['Mean_lag_1'] = df.groupby('pid')['Mean'].shift(1)
df['Std_Dev_lag_1'] = df.groupby('pid')['Std_Dev'].shift(1)
df['Arc_Length_lag_1'] = df.groupby('pid')['Arc_Length'].shift(1)

# Fill NaN values in lag features with the corresponding original feature value
df['Mean_lag_1'].fillna(df['Mean'], inplace=True)
df['Std_Dev_lag_1'].fillna(df['Std_Dev'], inplace=True)
df['Arc_Length_lag_1'].fillna(df['Arc_Length'], inplace=True)

# Replace missing numeric values with the mean of their respective columns
from sklearn.impute import SimpleImputer

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
imputer = SimpleImputer(strategy='mean')
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

# Define the features and target
features = [
    'Mean', 'Std_Dev', 'Arc_Length', 'Skewness_x', 'Kurtosis_x', 'HF_PSD', 'SampEn',
    'Number_of_SCRs', 'Summed_Magnitude_of_SCRs', 'Summed_Area_Under_SCRs',
    'Mean_Tonic_Level', 'Std_Tonic_Level', 'Mean_Phasic_Activity', 'Number_of_Responses',
    'Skewness_y', 'Kurtosis_y', 'AVNN', 'CV', 'SDNN', 'RMSSD', 'SDSD',
    'Mean_lag_1', 'Std_Dev_lag_1', 'Arc_Length_lag_1'
]
target = 'emotion'

# Calculate the correlation matrix and remove highly correlated features (threshold > 0.75)
corr_matrix = df[features].corr()
threshold = 0.75
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]

# Drop these features from the dataset
df_reduced = df.drop(columns=to_drop)
features_reduced = [feature for feature in features if feature not in to_drop]

# Filter out the 'sad' class and ensure balanced data
df_filtered = df_reduced[df_reduced['emotion'] != 'sad']

# Merge similar classes using .loc to avoid SettingWithCopyWarning
df_filtered.loc[:, 'emotion'] = df_filtered['emotion'].replace({
    'cheerful': 'happy',
    'nervous': 'angry'
})

# Filter the dataset to include only participants with at least one sample in at least two different classes
participant_class_counts = df_filtered.groupby('pid')['emotion'].value_counts().unstack().fillna(0)
balanced_participants = participant_class_counts[(participant_class_counts > 1).sum(axis=1) > 1].index.tolist()
df_balanced = df_filtered[df_filtered['pid'].isin(balanced_participants)]

# Define features, target, and groups after filtering
X = df_balanced[features_reduced]
y = df_balanced['emotion']
groups = df_balanced['pid']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Set up Leave-One-Group-Out cross-validation
logo = LeaveOneGroupOut()

# Define the parameter grid for RandomizedSearchCV
param_dist_rf = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize RandomForestClassifier and RandomizedSearchCV
rf = RandomForestClassifier(random_state=42, class_weight='balanced')
random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_dist_rf,
                                      n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)

# Lists to store results
classification_reports = []

# Perform LOSO cross-validation on balanced data
for train_index, test_index in logo.split(X, y_encoded, groups):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_encoded[train_index], y_encoded[test_index]

# Apply SMOTE to balance the training data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# Train model with RandomizedSearchCV
random_search_rf.fit(X_train_scaled, y_train_smote)

# Make predictions on the test set
y_pred_rf = random_search_rf.predict(X_test_scaled)

# Generate classification report
report = classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_, zero_division=0)
classification_reports.append(report)
print(f'Participant: {groups.iloc[test_index].iloc[0]} - Classification Report:\n{report}')

# Final participant counts after LOSO
print("Final participant counts:\n", groups.value_counts())

# Add the encoded emotion labels back to the DataFrame for easier plotting
df_balanced['emotion_encoded'] = y_encoded

# Create a pivot table to count the number of samples per class for each participant
class_counts = df_balanced.pivot_table(index='pid', columns='emotion', aggfunc='size', fill_value=0)

# Plotting the class distribution for each participant
plt.figure(figsize=(12, 8))
class_counts.plot(kind='bar', stacked=True, cmap='viridis', edgecolor='black')